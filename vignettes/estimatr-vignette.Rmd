---
title: "Estimatr: fast estimators for social scientists"
author: "Luke Sonnet"
bibliography: estimatr.bib
link-citations: true
date: "`r Sys.Date()`"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{Estimatr - Fast estimators for social scientists}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

\newcommand{\X}{\mathbf{X}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Gb}{\mathbf{G}}
\newcommand{\XtXinv}{(\X^{\top}\X)^{-1}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\V}{\mathbb{V}}

`estimatr` is a package in r dedicated to providing fast estimators for social scientists. Many extant estimators in base r or popular packages are slow, have default settings that lead to statistically inappropriate estimates, or are unable to provide certain specifications.

This vignette will demonstrate the various estimators available in this package and provide justifications and citations for the default settings. The estimators currently available are:

* [`lm_robust()`](#lm_robust) - for fitting linear models with heteroskedasticity/cluster robust standard errors
* [`lm_lin()`](#lm_lin) - a wrapper for `lm_robust()` to simplify interacting centered pre-treatment covariates with a treatment variable
* [`difference_in_means()`](#difference_in_means) - for estimating differences in means with appropriate standard errors for simple, cluster randomized, block randomized, matched-pair designs and more
* [`horvitz_thompson()`](#horvitz_thompson) - for estimating average treatment effects taking into consideration treatment probabilities or sampling probabilities for simple and cluster randomized designs

# Hypothetical experiment

TODO

# Estimators

## `lm_robust()`

All of the following works with weights except for the clustered standard errors (todo: finish weights with clustered SEs and write up weights in below.)

### Coefficient estimates

\[
\hat{\beta} =\XtXinv\X^{\top}\y
\]

### Variance

**Definitions**

* $\x_i$ is the $i$th row of $\X$.
* $h_{ii} = \x_i\XtXinv\x^{\top}_i$
* $e_i = y_i - \x_i\hat{\beta}$
* $\mathrm{diag}[.]$ is an operator that creates a diagonal matrix

#### Heteroskedasticity-Robust Variance

**HC2 (default)**
\[
\hat{\V}[\hat{\beta}]_{HC2} =  \XtXinv\X^{\top}\mathrm{diag}\left[\frac{e_i^2}{1-h_{ii}}\right]\X\XtXinv
\]

**HC0**
\[
\hat{\V}[\hat{\beta}]_{HC0} =  \XtXinv\X^{\top}\mathrm{diag}\left[e_i^2\right]\X\XtXinv
\]

**HC1**
\[
\hat{\V}[\hat{\beta}]_{HC1} =  \frac{N}{N-K}\XtXinv\X^{\top}\mathrm{diag}\left[e_i^2\right]\X\XtXinv
\]
where $N$ is the number of observations and $K$ is the number of elements in $\beta$.

**HC3**
\[
\hat{\V}[\hat{\beta}]_{HC3} =  \XtXinv\X^{\top}\mathrm{diag}\left[\frac{e_i^2}{(1-h_{ii})^2}\right]\X\XtXinv
\]

#### Cluster-Robust Variance

**Definitions**

* $S$ is the number of clusters
* $\X_s$ is the rows of $\X$ that belong to cluster $s$
* $\Pb_{ss} = \X_s \XtXinv \X^\top_s$
* $N_s$ is the number of units in cluster $s$
* $I_n$ is an identity matrix of size $n\times n$
* $\e_s$ is the elements of the residual matrix $\e$ in cluster $s$, or $\e_s = \y_s - \X_s \hat{\beta}$

**BM (default, also known as CR2 or Bell-McCaffrey)**

Analogy to HC2 for cluster-robust variance estimation. For the original reference, see [@bellmccaffrey2002]. For papers that have also mention it, see $\V_{\mathrm{LZ2}}$ on p. 709 of [@imbenskolesar2016] and the CR2 estimator shown in equations (4) and (5) in [@pustejovskytipton2016]. Note that we will be implementing the estimator proposed in [@pustejovskytipton2016] soon; more can be found on their GitHub page for an accompanying R package [clubSandwich](https://github.com/jepusto/clubSandwich).

\[
\hat{\V}[\hat{\beta}]_{BM} =  \XtXinv \sum^S_{s=1} \left[\X^\top_s (I_{N_s} - \Pb_{ss})^{-\frac{1}{2}} \e_s\e^\top_s (I_{N_s} - \Pb_{ss})^{-\frac{1}{2}} \X_s \right] \XtXinv
\]

**stata**

\[
\hat{\V}[\hat{\beta}]_{stata} = \frac{N-1}{N-K}\frac{S}{S-1} \XtXinv \sum^S_{s=1} \left[\X^\top_s \e_s\e^\top_s \X_s \right] \XtXinv
\]

#### Classical

**classical**
\[
\hat{\V}[\hat{\beta}]_{classical} =  \frac{\e^\top\e}{N-K} \XtXinv 
\]

### Confidence intervals and hypothesis testing

For confidence intervals and hypothesis testing, we use $t$-statistics and thus require degrees of freedom. For all specifications that *do not involve clustering*,

\[
df = N - K
\]

When there is clustering and `se_type = "stata"` and, we match Stata's conservative degrees of freedom by setting

\[
df_{stata} = S - 1
\]

When there is clustering and `se_type = "BM"`, we match Bell and McCaffrey's degrees of freedom adjustment (this is described in detail on p. 709 of [@imbenskolesar2016] as well as equation (11) in [@pustejovskytipton2016]). Where $z_{K,k}$ is a vector of length $K$ where the $k$th element is 1 and all other elements are 0. The $k$ signifies for which coefficient we are getting the degrees of freedom.

Also, $\Pb = \X\XtXinv\X^\top$ and $(I - \Pb)_s$ is the $N_s$ columns that correspond to cluster $s$.

We define $\Gb$ as an $N\times S$ matrix with each column as

\[
\Gb_s = (I-\Pb)_s (I_{N_s} - \Pb_{ss})^{-\frac{1}{2}} \X_s \XtXinv z_{K, k}
\]

Then the degrees of freedom for coefficient $k$ is

\[
df_{k, BM} = \frac{\left(\sum^S_{s=1} \lambda_i\right)^2}{\sum^S_{s=1} \lambda^2_i}
\]
where $\lambda_i$ are the eigenvalues of $\Gb^\top\Gb$.

Then, if $\hat{\V}_k$ is the $k$th diagonal element of $\hat{\V}$, we build confidence intervals using the user specified $\alpha$ as:

\[
\mathrm{CI}^{1-\alpha} = \left(\hat{\beta_k} + t^{df}_{\alpha/2} \sqrt{\hat{\V}_k}, \hat{\beta_k} + t^{df}_{1 - \alpha/2} \sqrt{\hat{\V}_k}\right)
\]

The associated p-values for a two-sided null hypothesis test where the null is that the coefficient is 0 uses a t-distribution with the aforementioned significance level $\alpha$ and degrees of freedom $df$.

## `lm_lin()`

The `lm_lin()` estimator is a data pre-processor for `lm_robust()` that implements the regression method for covariate adjustment suggested by [@lin2013]. In response to the critique by [@freedman2008] that using regression to adjust for pre-treatment covariates could bias estimates of treatment effects, [@lin2013] demonstrates the small magnitude of this bias while also presenting an estimator to reduce said bias.

This estimator takes the outcome and treatment variable as the main formula (`formula`) and takes a right-sided formula of all pre-treatment covariates (`covariates`). These pre-treatment covariates are then centered to be mean zero and interacted with the treatment variable before being added to the formula and passed to `lm_robust()`.

The rest of the estimation proceeds just as in [`lm_robust()`](#lm_robust).

## `difference_in_means()`

There are six kinds of experimental designs for which our `difference_in_means()` estimator can estimate treatment effects, provide estiamtes of uncertainty, construct confidence intervals, and 
provide p-values. We list them here along with how the software learns the design:

* Simple (both `cluster_variable_name` and `block_variable_name` are unused)
* Clustered (`cluster_variable_name` is specified while `block_variable_name` is not)
* Blocked (`block_variable_name` is specified while `cluster_variable_name` is not)
* Blocked and clustered (both are specified)

There are two subsets of blocked designs that we also consider:

* Matched-pairs (only `block_variable_name` is specified and all blocks are size two)
* Matched-pair clustered design (both names are specified and each block only has two clusters)

In addition, weights can be specified.

### Estimates

**Any unblocked design**
\[
\hat{\tau} = \frac{1}{N} \sum^N_{i=1} z_i y_i - (1 - z_i) y_i
\]
where $z_i$ is the treatment variable, $y_i$ is the outcome, and $N$ is the total number of units.

**Blocked design (including matched-pairs designs)**
\[
\hat{\tau} = \sum^J_{j=1} \frac{N_j}{N} \hat{\tau_j}
\]
where $J$ is the number of blocks, $N_j$ is the size of those blocks, and $\hat{\tau_j}$ is the estimated difference-in-means in block $j$.

### Variance

**Simple**
\[
\hat{\V}[\hat{\tau}] = \frac{\hat{\V}[y_{i,0}]}{N_0} + \frac{\hat{\V}[y_{i,1}]}{N_1}
\]
where $\hat{\V}[y_{i,k}]$ is the Bessel-corrected variance of all units where $z_i = k$ and $N_k$ is the number of units in condition $k$.

**Clustered**
\[
\hat{\V}[\hat{\tau}] = \frac{N\hat{\V}[y_{i,0}]}{SN_0} + \frac{N\hat{\V}[y_{i,1}]}{SN_1}
\]
where $S$ is the number of clusters. See equation 3.23 on page 83 of [@gerbergreen2012] for a reference.

**Blocked & blocked and clustered**
\[
\hat{\V}[\hat{\tau}] = \sum^J_{j=1} \left(\frac{N_j}{N}\right)^2 \hat{\V}[\hat{\tau_j}] 
\]
where $\hat{\V}[\hat{\tau_j}]$ is the variance of the estimated difference-in-means in block $j$. See footnote 17 on page 74 of [@gerbergreen2012] for a reference.

**Matched-pairs**
\[
\hat{\V}[\hat{\tau}] = \frac{1}{J(J-1)} \sum^J_{j=1} \left(\hat{\tau_j} - \hat{\tau}\right)^2 
\]
See equation 3.16 on page 77 of [@gerbergreen2012] for a reference.

**Matched-pair cluster randomized**
This formula is the variance of the SATE defined in Equation 6 on page 36 of [@imaietal2009].
\[
\hat{\V}[\hat{\tau}] = \frac{J}{(J-1)N^2} \sum^J_{j=1} \left(N_j \hat{\tau_j} - \frac{N \hat{\tau}}{J}\right)^2
\]

### Confidence intervals and hypothesis testing

In order to estimate confidence intervals and produce p-values, we first must estimate the degrees of freedom.

**Simple**
We use the well known Welch-Satterthwaite equation to estimate the degrees of freedom for simple designs.
\[
df = \frac{(\hat{\V}[\hat{\tau}])^2}{\frac{(\hat{\V}[y_{i,0}])^2}{N^2_0(N_0-1)} + \frac{(\hat{\V}[y_{i,1}])^2}{N^2_1(N_1-1)}}
\]

**Clustered**
We use the same degrees of freedom above for cluster randomized experiments. However, [@gerbergreen2012] recommend in footnote 20 on page 83 that the bounds of the confidence interval must be expanded by $\sqrt{(J-1)/(J-2)}$. We do not implement this. TODO: instead of expanding confidence intervals, find equivalent degrees of freedom correction so that p-values and hypothesis tests are in accordance.

**Blocked & block-clustered & matched-pair**
TODO: check best practices on degrees of freedom with blocks. Here we set
\[
df = N - J - 1
\]
which matches the degrees of freedom when using regression to estimate treatment effects with dummy variables for each block.

**Matched-pair cluster randomized**
Following advice of [@imaietal2009] on page 37, we use the following conservative estimate of the degrees of freedom:
\[
df_{MPCR} = J - 1
\]

## `horvitz_thompson()`

TODO

# References
