<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How Stata’s hat matrix differs with weights • estimatr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../">estimatr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/getting-started.html">Getting Started</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Technical Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/mathematical-notes.html">Mathematical Notes</a>
    </li>
    <li>
      <a href="../articles/benchmarking-estimatr.html">Simulations - Speed Comparisons</a>
    </li>
    <li>
      <a href="../articles/simulations-ols-variance.html">Simulations - OLS and Variance</a>
    </li>
    <li>
      <a href="../articles/simulations-debiasing-dim.html">Simulations - Debiasing Difference-in-Means</a>
    </li>
    <li>
      <a href="../articles/stata-wls-hat.html">How Stata's hat matrix differs with weights</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="http://discuss.declaredesign.org">Ask for Help</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="http://declaredesign.org">DeclareDesign</a>
    </li>
    <li>
      <a href="http://randomizr.declaredesign.org">randomizr</a>
    </li>
    <li>
      <a href="http://fabricatr.declaredesign.org">fabricatr</a>
    </li>
    <li>
      <a href="http://estimatr.declaredesign.org">estimatr</a>
    </li>
  </ul>
</li>
<li>
  <a href="http://declaredesign.org">DeclareDesign home</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a></a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>How Stata’s hat matrix differs with weights</h1>
                        <h4 class="author">Luke Sonnet</h4>
            
          </div>

    
    
<div class="contents">
<p>Researchers use linear regression with heteroskedasticity-robust standard errors. Many social scientists use either Stata or R. One would hope the two would always agree in their estimates. Unfortunately, estimating weighted least squares with HC2 or HC3 robust variance results in different answers across Stata and common approaches in R as well as Python.</p>
<p>The discrepancy is due to differences in how the software estimates the “hat” matrix, on which both HC2 and HC3 variance estimators rely. The short story is that Stata estimates the hat matrix as</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top
\]</span></p>
<p>while the usual approaches in R, including <a href="https://CRAN.R-project.org/package=sandwich"><code>sandwich</code></a> and <a href="/R/estimatr/"><code>estimatr</code></a>, and Python (e.g. <a href="http://www.statsmodels.org/stable/index.html"><code>statsmodels</code></a>) estimate the following hat matrix</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}
\]</span></p>
<p>This results in differences when researches estimate HC2 and HC3 variance estimators. The HC1 standard errors, Stata’s default, are the same across all packages. The rest of this document just walks through the set-up for the above and demonstrates some results from Stata, R, and Python.</p>
<div id="weighted-least-squares" class="section level3">
<h3 class="hasAnchor">
<a href="#weighted-least-squares" class="anchor"></a>Weighted least squares</h3>
<p>Let’s briefly review WLS. Weights are used in linear regression often for two key problems; (1) to model and correct for heteroskedasticity, and (2) to deal with unequal sampling (or treatment) probabilities. In both cases, we take the standard model</p>
<p><span class="math display">\[
y_i = \mathbf{x}_i^\top \mathbf{\beta} + \epsilon_i,
\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the <span class="math inline">\(i\)</span>th unit’s outcome, <span class="math inline">\(\mathbf{x}_i\)</span> is a column vector of covariates, <span class="math inline">\(\mathbf{\beta}\)</span> are the coefficients of interest, and <span class="math inline">\(\epsilon\)</span> is some error, and rescale the model by the square root of that unit’s weight, <span class="math inline">\(\sqrt{w_i}\)</span>. Our model then becomes</p>
<p><span class="math display">\[
\frac{y_i}{\sqrt{w_i}} = \frac{\mathbf{x}_i^\top}{\sqrt{w_i}} \mathbf{\beta} + \frac{\epsilon_i}{\sqrt{w_i}}.
\]</span></p>
<p>It can be shown that the solution for <span class="math inline">\(\mathbf{\beta}\)</span> is</p>
<p><span class="math display">\[
\widehat{\mathbf{\beta}} = (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{W}\)</span> is a diagonal matrix where each entry is <span class="math inline">\(w_{i}\)</span>, <span class="math inline">\(\mathbf{X}\)</span> is the covariate matrix, and <span class="math inline">\(\mathbf{y}\)</span> is the outcome column vector. Note that all weights have been scaled to sum to 1 (i.e., <span class="math inline">\(\sum_i w_{ii} = 1\)</span>). An easy way to get to compute <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> is to first weight both <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> by <span class="math inline">\(\mathbf{W}^s\)</span>, which is simply the weight matrix but using instead the square root of the weights. Let’s define these rescaled matrices as</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\mathbf{X}} &amp;= \mathbf{X} \mathbf{W}^s \\
\widetilde{\mathbf{y}} &amp;= \mathbf{W}^s \mathbf{y}
\end{aligned}
\]</span></p>
</div>
<div id="heteroskedastic-consistent-variance-estimators" class="section level3">
<h3 class="hasAnchor">
<a href="#heteroskedastic-consistent-variance-estimators" class="anchor"></a>Heteroskedastic-consistent variance estimators</h3>
<p>Turning to variance, the standard sandwich estimator is</p>
<p><span class="math display">\[
\mathbb{V}[\widehat{\mathbf{\beta}}] = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \Omega \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1}
\]</span></p>
<p>where <span class="math inline">\(\Omega\)</span> represents <span class="math inline">\(\mathbb{E}[\mathbf{\epsilon}\mathbf{\epsilon}^\top]\)</span>, the variance-covariance matrix of the disturbances. A nice review of the different variance estimators along with their properties can be found in <span class="citation">Long and Ervin (<a href="#ref-longervin2000">2000</a>)</span> <a href="http://www.indiana.edu/~jslsoc/files_research/testing_tests/hccm/99TAS.pdf">[ungated]</a>. The HC2 and HC3 estimators, introduced by <span class="citation">MacKinnon and White (<a href="#ref-mackinnonwhite1985">1985</a>)</span>, use the hat matrix as part of the estimation of <span class="math inline">\(\Omega\)</span>. The standard hat matrix is written:</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top
\]</span></p>
<p>Where <span class="math inline">\(h_{ii}\)</span> are the diagonal elements of the hat matrix, the HC2 variance estimator is</p>
<p><span class="math display">\[
\mathbb{V}[\widehat{\mathbf{\beta}}]_{HC2} = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \mathrm{diag}\left[\frac{e^2_i}{1 - h_{ii}}\right] \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1}  ,
\]</span></p>
<p>where <span class="math inline">\(e_i\)</span> are the residuals. The HC3 estimator is very similar,</p>
<p><span class="math display">\[
\mathbb{V}[\widehat{\mathbf{\beta}}]_{HC2} = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \mathrm{diag}\left[\frac{e^2_i}{1 - (h_{ii})^2}\right] \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1} .
\]</span></p>
<p>Both rely on the hat matrix. Crucially, this is where Stata and the packages and modules in R and Python disagree. When weights are specified, Stata estimates the hat matrix as</p>
<p><span class="math display">\[
\mathbf{H}_{Stata} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top,
\]</span></p>
<p>while the other software uses</p>
<p><span class="math display">\[
\mathbf{H}_{R} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}.
\]</span></p>
<p>Thus the HC2 and HC3 estimator differ as the values of <span class="math inline">\(h_{ii}\)</span> are quite different. How different are these results? Let’s use a little example using <code>mtcars</code> a dataset included with R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Using estimatr</span>
<span class="kw">library</span>(estimatr)
<span class="kw"><a href="../reference/lm_robust.html">lm_robust</a></span>(
  mpg <span class="op">~</span><span class="st"> </span>hp,
  <span class="dt">data =</span> mtcars,
  <span class="dt">weights =</span> wt,
  <span class="dt">se_type =</span> <span class="st">"HC2"</span>
)
<span class="co">#&gt;                Estimate Std. Error     Pr(&gt;|t|)    CI Lower    CI Upper DF</span>
<span class="co">#&gt; (Intercept) 28.54864505 2.16281844 4.975934e-14 24.13158053 32.96570958 30</span>
<span class="co">#&gt; hp          -0.06249413 0.01445662 1.561752e-04 -0.09201849 -0.03296977 30</span></code></pre></div>
<p>We can also see that Python’s <a href="http://www.statsmodels.org/stable/index.html"><code>statsmodels</code></a> provides the same results as the methods in R (and in fact they note the difference in an <a href="https://github.com/statsmodels/statsmodels/issues/1209">issue on GitHub</a>).</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> statsmodels.api <span class="im">as</span> sm
<span class="im">import</span> pandas <span class="im">as</span> pd
dat <span class="op">=</span> pd.read_csv(<span class="st">'mtcars.csv'</span>)
wls_mod <span class="op">=</span> sm.WLS(dat[<span class="st">'mpg'</span>], sm.add_constant(dat[<span class="st">'hp'</span>]), weights <span class="op">=</span> dat[<span class="st">'wt'</span>])
<span class="bu">print</span>(wls_mod.fit().HC2_se)
<span class="co">#&gt; const    2.162818</span>
<span class="co">#&gt; hp       0.014457</span>
<span class="co">#&gt; dtype: float64</span></code></pre></div>
<p>If we do the same in Stata 13, we get the following output:</p>
<pre class="stata"><code>insheet using mtcars.csv
reg mpg hp [aweight=wt], vce(hc2)</code></pre>
<pre><code>Linear regression                                      Number of obs =      32
                                                       F(  1,    30) =   19.08
                                                       Prob &gt; F      =  0.0001
                                                       R-squared     =  0.5851
                                                       Root MSE      =  3.6191

------------------------------------------------------------------------------
             |             Robust HC2
         mpg |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          hp |  -.0624941   .0143083    -4.37   0.000    -.0917155   -.0332727
       _cons |   28.54865   2.155169    13.25   0.000      24.1472    32.95009
------------------------------------------------------------------------------</code></pre>
<p>Stata’s standard errors are somewhat different. The only documentation of Stata’s formula for the hat matrix can be found on the <a href="https://www.statalist.org/forums/forum/general-stata-discussion/general/329653-regress-postestimation-with-weights">statalist forum here</a> and nowhere in the official documentation as far as I can tell.</p>
<div id="which-should-we-prefer" class="section level4">
<h4 class="hasAnchor">
<a href="#which-should-we-prefer" class="anchor"></a>Which should we prefer?</h4>
<p>Just because Stata is not documenting their HC2 and HC3 estimator does not mean they’re wrong. Also the differences tend to be minor. In fact, it is unclear which we should prefer given that there is not a strong literature supporting one or the other. However, there are several arguments to be made for <span class="math inline">\(\matbf{H}_{R}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>It’s the estimator you get when you weight your data by the square root of the weights (<span class="math inline">\(\mathbf{X} \rightarrow \widetilde{\mathbf{X}}\)</span> and <span class="math inline">\(\mathbf{y} \rightarrow \widetilde{\mathbf{y}}\)</span>) and fit regular ordinary least squares. If one considers the weighted model as simply a rescaled version of the unweighted model, then users should prefer <span class="math inline">\(\mathbf{H}_{R}\)</span>.</li>
<li>The diagonal of <span class="math inline">\(\mathbf{H}_{R}\)</span> are the weighted leverages <span class="citation">(Li and Valliant <a href="#ref-livalliant2009">2009</a>)</span>, while <span class="math inline">\(\mathbf{H}_{Stata}\)</span> would need to be weighted again for the diagonal to recover the weighted leverage.</li>
</ol>
</div>
</div>
<div id="references" class="section level3 unnumbered">
<h3 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h3>
<div id="refs" class="references">
<div id="ref-livalliant2009">
<p>Li, Jianzhu, and Richard Valliant. 2009. “Survey Weighted Hat Matrix and Leverages.” <em>Survey Methodology</em> 35 (1). Statistics Canada: 15–24.</p>
</div>
<div id="ref-longervin2000">
<p>Long, J Scott, and Laurie H Ervin. 2000. “Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.” <em>The American Statistician</em> 54 (3). Taylor &amp; Francis Group: 217–24. <a href="https://doi.org/10.1080/00031305.2000.10474549" class="uri">https://doi.org/10.1080/00031305.2000.10474549</a>.</p>
</div>
<div id="ref-mackinnonwhite1985">
<p>MacKinnon, James, and Halbert White. 1985. “Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.” <em>Journal of Econometrics</em> 29 (3): 305–25. <a href="https://doi.org/10.1016/0304-4076(85)90158-7" class="uri">https://doi.org/10.1016/0304-4076(85)90158-7</a>.</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright" style="flex:3;">
  <p>Developed by Graeme Blair, Jasper Cooper, Alexander Coppock, Macartan Humphreys, Luke Sonnet, Neal Fultz.</p>
  <p>Code is licensed under <a href="https://opensource.org/licenses/mit-license.php">MIT</a> license.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
